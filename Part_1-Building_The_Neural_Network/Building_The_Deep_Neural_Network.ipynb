{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE4eeh7sdneI"
      },
      "source": [
        "# Building a Deep Neural Network:\n",
        "\n",
        "- Implementing functions required to build a deep neural network.\n",
        "- I'll then use these functions to build a deep neural network for image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maiRTG-tdneK"
      },
      "source": [
        "## Table of Contents\n",
        "- [1 - Packages](#1)\n",
        "- [2 - Outline](#2)\n",
        "- [3 - Initialization](#3)\n",
        "    - [3.1 - 2-layer Neural Network](#3-1)\n",
        "        - [Step 1 - initialize_parameters](#step-1)\n",
        "    - [3.2 - L-layer Neural Network](#3-2)\n",
        "        - [Step 2 - initialize_parameters_deep](#step-2)\n",
        "- [4 - Forward Propagation Module](#4)\n",
        "    - [4.1 - Linear Forward](#4-1)\n",
        "        - [Step 3 - linear_forward](#step-3)\n",
        "    - [4.2 - Linear-Activation Forward](#4-2)\n",
        "        - [Step 4 - linear_activation_forward](#step-4)\n",
        "    - [4.3 - L-Layer Model](#4-3)\n",
        "        - [Step 5 - L_model_forward](#step-5)\n",
        "- [5 - Cost Function](#5)\n",
        "    - [Step 6 - compute_cost](#step-6)\n",
        "- [6 - Backward Propagation Module](#6)\n",
        "    - [6.1 - Linear Backward](#6-1)\n",
        "        - [Step 7 - linear_backward](#step-7)\n",
        "    - [6.2 - Linear-Activation Backward](#6-2)\n",
        "        - [Step 8 - linear_activation_backward](#step-8)\n",
        "    - [6.3 - L-Model Backward](#6-3)\n",
        "        - [Step 9 - L_model_backward](#step-9)\n",
        "    - [6.4 - Update Parameters](#6-4)\n",
        "        - [Step 10 - update_parameters](#step-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwhuhdlYdneK"
      },
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soz4QceadneL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
        "\n",
        "\n",
        "import copy\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49Gm8zS0dneM"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Outline\n",
        "\n",
        "To build the neural network, I'll implement several \"helper functions.\" These helper functions will be used in the 2nd part of the project to build a two-layer neural network and an L-layer neural network.\n",
        "\n",
        "- Initialization of the parameters for a two-layer network and for an $L$-layer neural network\n",
        "\n",
        "- Implementation of the forward propagation module (shown in purple in the figure below)\n",
        "     - LINEAR part of a layer's forward propagation step (resulting in $Z^{[l]}$).\n",
        "     - ACTIVATION function  (relu/sigmoid)\n",
        "     - [LINEAR->ACTIVATION] forward function\n",
        "     - Stacking [LINEAR->RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR->SIGMOID] at the end (for the final layer $L$)\n",
        "\n",
        "- Computation of the loss\n",
        "\n",
        "- Implementation of the backward propagation module (denoted in red in the figure below)\n",
        "    - LINEAR part of a layer's backward propagation step\n",
        "    - Gadient of the ACTIVATION function (relu_backward/sigmoid_backward)\n",
        "    - [LINEAR->ACTIVATION] backward function\n",
        "    - Stacking [LINEAR->RELU] backward L-1 times and add [LINEAR->SIGMOID] backward in a new L_model_backward function\n",
        "\n",
        "- Update of the parameters\n",
        "\n",
        "\n",
        "For every forward function, there is a corresponding backward function. This is why at every step of your forward module you will be storing some values in a cache. These cached values are useful for computing gradients.\n",
        "\n",
        "In the backpropagation module, we will then use the cache to calculate the gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M_Hmk3FdneM"
      },
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Initialization\n",
        "\n",
        "<a name='3-1'></a>\n",
        "### 3.1 - 2-layer Neural Network\n",
        "\n",
        "<a name='step-1'></a>\n",
        "### Step 1 - initialize_parameters\n",
        "\n",
        "Creates and initializes the parameters of the 2-layer neural network.\n",
        "\n",
        "The model's structure is: *LINEAR -> RELU -> LINEAR -> SIGMOID*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c468c89deb6d0cacf2ade5ab4151d26e",
          "grade": false,
          "grade_id": "cell-96d4e144d9419b32",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "il2KK8aDdneM"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "\n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykYs6M97dneN"
      },
      "source": [
        "<a name='3-2'></a>\n",
        "### 3.2 - L-layer Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qsq4AkCdneO"
      },
      "source": [
        "<a name='step-2'></a>\n",
        "### Step 2 -  initialize_parameters_deep\n",
        "\n",
        "Initialization for an L-layer Neural Network.\n",
        "\n",
        "The model's structure is *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1773f5c69d941998dc8da88f4151e8d3",
          "grade": false,
          "grade_id": "cell-37b22e0664a4949e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8M0pIK0WdneO"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims) # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1_qZHXbdneP"
      },
      "source": [
        "<a name='4'></a>\n",
        "## 4 - Forward Propagation Module\n",
        "\n",
        "<a name='4-1'></a>\n",
        "### 4.1 - Linear Forward\n",
        "\n",
        "Now that we have initialized your parameters, we can do the forward propagation module. Now, we'll complete three functions in this order:\n",
        "\n",
        "- LINEAR\n",
        "- LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.\n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (whole model)\n",
        "\n",
        "The linear forward module computes the following equations:\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
        "\n",
        "where $A^{[0]} = X$.\n",
        "\n",
        "<a name='step-3'></a>\n",
        "### Step 3 - linear_forward\n",
        "\n",
        "Linear part of forward propagation.\n",
        "\n",
        "The mathematical representation of this unit is $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "770763ab229ee87e8f5dfd520428caa3",
          "grade": false,
          "grade_id": "cell-4d6e09486a53f4c4",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "MoJE9G5kdneP"
      },
      "outputs": [],
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter\n",
        "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    Z = np.dot(W, A) + b\n",
        "\n",
        "    cache = (A, W, b)\n",
        "\n",
        "    return Z, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERm33802dneQ"
      },
      "source": [
        "<a name='4-2'></a>\n",
        "### 4.2 - Linear-Activation Forward\n",
        "\n",
        "We will use two activation functions:\n",
        "\n",
        "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. `sigmoid` function returns **two** items: the activation value \"`a`\" and a \"`cache`\" that contains \"`Z`\".\n",
        "``` python\n",
        "A, activation_cache = sigmoid(Z)\n",
        "```\n",
        "\n",
        "- **ReLU**: The mathematical formula for ReLu is $A = RELU(Z) = max(0, Z)$. `relu` function returns **two** items: the activation value \"`A`\" and a \"`cache`\" that contains \"`Z`\".\n",
        "``` python\n",
        "A, activation_cache = relu(Z)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egFSorgcdneQ"
      },
      "source": [
        "<a name='step-4'></a>\n",
        "### Step 4 - linear_activation_forward\n",
        "\n",
        "Lets' implement the forward propagation of the *LINEAR->ACTIVATION* layer. Mathematical relation is: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ where the activation \"g\" can be sigmoid() or relu()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f09e76f2a56c8ee77db3e89214a676b2",
          "grade": false,
          "grade_id": "cell-eb48903dd8e48a90",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1P9WWtw_dneQ"
      },
      "outputs": [],
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value\n",
        "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "\n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prLcSm3jdneR"
      },
      "source": [
        "<a name='4-3'></a>\n",
        "### 4.3 - L-Layer Model\n",
        "\n",
        "For even *more* convenience when implementing the $L$-layer Neural Net, we will implement a function that replicates the previous one (`linear_activation_forward` with RELU) $L-1$ times, then follows that with one `linear_activation_forward` with SIGMOID.\n",
        "\n",
        "<a name='step-5'></a>\n",
        "### Step 5 -  L_model_forward\n",
        "\n",
        "Let's implement the forward propagation of the above model.\n",
        "\n",
        "`AL` also often called `Yhat` denotes $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a0071c19f83d4b851dc8a67e66545262",
          "grade": false,
          "grade_id": "cell-9a8ec52ec8f6e04a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "kNvugSPjdneR"
      },
      "outputs": [],
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "\n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "\n",
        "    Returns:\n",
        "    AL -- activation value from the output (last) layer\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "\n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
        "        caches.append(cache)\n",
        "\n",
        "\n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    return AL, caches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJSbPWFTdneS"
      },
      "source": [
        "We've implemented a full forward propagation that takes the input X and outputs a row vector $A^{[L]}$ containing the predictions. It also records all intermediate values in \"caches\". Using $A^{[L]}$, we can compute the cost of your predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoKjWrIjdneS"
      },
      "source": [
        "<a name='5'></a>\n",
        "## 5 - Cost Function\n",
        "\n",
        "Now we can implement forward and backward propagation and we need to compute the cost, in order to check whether your model is actually learning.\n",
        "\n",
        "<a name='step-6'></a>\n",
        "### Step 6 - compute_cost\n",
        "Cross-entropy cost $J$: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "17919bb7d82635554b52aed7e96e8d9b",
          "grade": false,
          "grade_id": "cell-abad606772066f14",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Tj5QY1zJdneS"
      },
      "outputs": [],
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = (-1/m)*np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\n",
        "\n",
        "    cost = np.squeeze(cost)      # To make sure the cost's shape is what we expect (this turns [[17]] into 17).\n",
        "\n",
        "\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp-dduYxdneS"
      },
      "source": [
        "<a name='6'></a>\n",
        "## 6 - Backward Propagation Module\n",
        "\n",
        "We'll now implement helper functions for backpropagation.\n",
        "\n",
        "Now, similarly to forward propagation, we're going to build the backward propagation in three steps:\n",
        "1. LINEAR backward\n",
        "2. LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
        "3. [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (whole model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIn85hIGdneT"
      },
      "source": [
        "<a name='6-1'></a>\n",
        "### 6.1 - Linear Backward\n",
        "\n",
        "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
        "\n",
        "From $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$, we want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
        "\n",
        "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.\n",
        "\n",
        "Formulas:\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
        "\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
        "\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYs61eBIdneT"
      },
      "source": [
        "<a name='step-7'></a>\n",
        "### Step 7 - linear_backward\n",
        "\n",
        "Implementation of `linear_backward()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "137d11e28068848079eb6c315a59f2be",
          "grade": false,
          "grade_id": "cell-418e156a9203fe72",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "i4ByFpIydnec"
      },
      "outputs": [],
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDPus3A8dned"
      },
      "source": [
        "<a name='6-2'></a>\n",
        "### 6.2 - Linear-Activation Backward\n",
        "\n",
        "Next, we will create a function that merges the two helper functions: **`linear_backward`** and the backward step for the activation **`linear_activation_backward`**.\n",
        "\n",
        "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. You can call it as follows:\n",
        "\n",
        "```python\n",
        "dZ = sigmoid_backward(dA, activation_cache)\n",
        "```\n",
        "\n",
        "- **`relu_backward`**: Implements the backward propagation for RELU unit. You can call it as follows:\n",
        "\n",
        "```python\n",
        "dZ = relu_backward(dA, activation_cache)\n",
        "```\n",
        "\n",
        "<a name='step-8'></a>\n",
        "### Step 8 -  linear_activation_backward\n",
        "\n",
        "Implementation of the backpropagation for the *LINEAR->ACTIVATION* layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3497ac4aa36a57278edbfb84a44e1d72",
          "grade": false,
          "grade_id": "cell-6c59263d69168c17",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "NMwCIBpedned"
      },
      "outputs": [],
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l\n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "\n",
        "    return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsO1l--bdnee"
      },
      "source": [
        "<a name='6-3'></a>\n",
        "### 6.3 - L-Model Backward\n",
        "\n",
        "Now we will implement the backward function for the whole network.\n",
        "\n",
        "In the `L_model_backward` function, we'll iterate through all the hidden layers backward, starting from layer $L$. On each step, you will use the cached values for layer $l$ to backpropagate through layer $l$.\n",
        "\n",
        "**Initializing backpropagation**:\n",
        "\n",
        "To backpropagate through this network, the output is:\n",
        "$A^{[L]} = \\sigma(Z^{[L]})$. We need to compute `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
        "\n",
        "Formula:\n",
        "```python\n",
        "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "```\n",
        "\n",
        "We can then use this post-activation gradient `dAL` to keep going backward.\n",
        "\n",
        "<a name='step-9'></a>\n",
        "### Step 9 -  L_model_backward\n",
        "\n",
        "Implementation of the backpropagation for the *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d3e23a2b5f3b33e264a122b3c4b0d760",
          "grade": false,
          "grade_id": "cell-9eec96b6d83ff809",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "dUN_lQwZdnee"
      },
      "outputs": [],
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "\n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ...\n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "\n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "\n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1]\n",
        "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
        "    grads[\"dW\" + str(L)] = dW_temp\n",
        "\n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGzlni0gdnef"
      },
      "source": [
        "<a name='6-4'></a>\n",
        "### 6.4 - Update Parameters\n",
        "\n",
        "In this section, we'll update the parameters of the model, using gradient descent:\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
        "\n",
        "where $\\alpha$ is the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulEMGpGzdnef"
      },
      "source": [
        "<a name='step-10'></a>\n",
        "### Step 10 - update_parameters\n",
        "\n",
        "Implementation of the `update_parameters()` to update the parameters using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e5e27794edcc10ab613c3eecd83a3011",
          "grade": false,
          "grade_id": "cell-3cb535f16aba3339",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "fafSZ1Zrdnef"
      },
      "outputs": [],
      "source": [
        "def update_parameters(params, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "\n",
        "    Arguments:\n",
        "    params -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "                  parameters[\"W\" + str(l)] = ...\n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    parameters = copy.deepcopy(params)\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = params[\"W\" + str(l+1)] - learning_rate * grads[\"dW\"+ str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = params[\"b\" + str(l+1)] - learning_rate * grads['db'+ str(l+1)]\n",
        "\n",
        "    return parameters"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "c4HO0",
      "launcher_item_id": "lSYZM"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}